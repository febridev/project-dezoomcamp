{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/local/spark/python/pyspark/__init__.py'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/04 07:49:25 WARN Utils: Your hostname, sparkdev resolves to a loopback address: 127.0.1.1; using 10.0.2.23 instead (on interface enp0s3)\n",
      "22/04/04 07:49:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/febridev/spark/spark-3.0.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/04/04 07:49:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf() \\\n",
    "    .setMaster('local[*]') \\\n",
    "    .setAppName('test') \\\n",
    "    .set(\"spark.jars\", \"../spark/resources/jars/gcs-connector-hadoop3-latest.jar,../spark/resources/jars/spark-bigquery-latest_2.12.jar\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"/.google/credentials/google_credentials.json\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.json.keyfile\", \"/.google/credentials/google_credentials.json\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder \\\n",
    "#   .master(\"local[*]\") \\\n",
    "#   .appName('1.2. BigQuery Storage & Spark SQL - Python') \\\n",
    "#   .config('spark.jars', '/home/jovyan/work/jars/spark-bigquery-latest.jar') \\\n",
    "#   .getOrCreate()\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_artists = spark.read.option(\"header\",True).parquet(\"gs://dtc_data_lake_applied-mystery-341809/raw/artists.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- followers: double (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- popularity: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_artists.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_artists = types.StructType([\n",
    "\ttypes.StructField('id',types.StringType(),True),\n",
    "\ttypes.StructField('followers',types.DoubleType(),True),\n",
    "\ttypes.StructField('name',types.StringType(),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artists = spark.read \\\n",
    ".option(\"header\",True) \\\n",
    ".schema(schema_artists) \\\n",
    ".parquet(\"gs://dtc_data_lake_applied-mystery-341809/raw/artists.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artists = df_artists.repartition(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# df_artists = spark.read \\\n",
    "# .option(\"header\",True) \\\n",
    "# .parquet(\"gs://dtc_data_lake_applied-mystery-341809/transform/artists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+\n",
      "|                  id|followers|                name|\n",
      "+--------------------+---------+--------------------+\n",
      "|59qz10hjQPAs0spos...|     18.0|    Boys of the Band|\n",
      "|0LcrfJ63GdIW4n2UZ...|      0.0|David Mackersie e...|\n",
      "|18lEjlk2JBPdyObT3...|    785.0|Ministerio Doble ...|\n",
      "|1UhZGGHbPPHAHt2ce...|    214.0|  Andre Tschaskowski|\n",
      "|5D95DPWHohrzVbdub...|  10557.0|       Advent Sorrow|\n",
      "|11CbG4ImkEw99aUng...|   3614.0|                  MX|\n",
      "|61MH29rMIyOfuK7KX...|  27878.0| The Vintage Caravan|\n",
      "|5lInFfKjIAVzBIOg4...|    445.0|     Charles Grigsby|\n",
      "|3kJakFcxRwLW9f47x...|      4.0|        Willy Rustad|\n",
      "|508weSx4HBumrGggF...|   1110.0|              Hunxho|\n",
      "+--------------------+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df_artists.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Update to your GCS bucket\n",
    "gcs_bucket = 'dtc_data_lake_applied-mystery-341809'\n",
    "\n",
    "# Update to your BigQuery dataset name you created\n",
    "bq_dataset = 'bq_project_dezoomcamp'\n",
    "\n",
    "# Enter BigQuery table name you want to create or overwite. \n",
    "# If the table does not exist it will be created when you run the write function\n",
    "bq_table = 'dim_artists'\n",
    "\n",
    "df_artists.write \\\n",
    "  .format(\"bigquery\") \\\n",
    "  .option(\"table\",\"{}.{}\".format(bq_dataset, bq_table)) \\\n",
    "  .option(\"temporaryGcsBucket\", gcs_bucket) \\\n",
    "  .mode('overwrite') \\\n",
    "  .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
